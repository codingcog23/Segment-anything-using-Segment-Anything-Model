# -*- coding: utf-8 -*-
"""how_to_segment_anything_with_sam_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17SZVDExsuyuHCMcysvygX5Q-oC6Lp6DL

# Segment Anything Model (SAM)

---

[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything) [![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643)

Segment Anything Model (SAM): a new AI model from Meta AI that can "cut out" any object, in any image, with a single click. SAM is a promptable segmentation system with zero-shot generalization to unfamiliar objects and images, without the need for additional training. This notebook is an extension of the [official notebook](https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb) prepared by Meta AI.

![segment anything model](https://media.roboflow.com/notebooks/examples/segment-anything-model-paper.png)


## Pro Tip: Use GPU Acceleration

If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.

## Steps in this Tutorial

In this tutorial, we are going to cover:

- **Before you start** - Make sure you have access to the GPU
- Install Segment Anything Model (SAM)
- Download Example Data
- Load Model
- Automated Mask Generation
- Generate Segmentation with Bounding Box
- Segment Anything in Roboflow Universe Dataset

## Let's begin!

## Before you start

Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`.
"""

!nvidia-smi

"""**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."""

from google.colab import drive
drive.mount('/content/drive')

import os
HOME = os.getcwd()
print("HOME:", HOME)

"""## Install Segment Anything Model (SAM) and other dependencies"""

# Commented out IPython magic to ensure Python compatibility.
# %cd {HOME}

import sys
!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'

!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision

"""### Download SAM weights"""

# Commented out IPython magic to ensure Python compatibility.
# %cd {HOME}
!mkdir {HOME}/weights
# %cd {HOME}/weights

!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

import os

CHECKPOINT_PATH = os.path.join(HOME, "weights", "sam_vit_h_4b8939.pth")
print(CHECKPOINT_PATH, "; exist:", os.path.isfile(CHECKPOINT_PATH))

"""## Download Example Data

**NONE:** Let's download few example images. Feel free to use your images or videos.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd {HOME}
!mkdir {HOME}/data
# %cd {HOME}/data

!wget -q https://github.com/codingcog23/Segment-anything-using-Segment-Anything-Model/blob/main/data/humans.jpg
!wget -q https://github.com/codingcog23/Segment-anything-using-Segment-Anything-Model/blob/main/data/dog.jpeg
!wget -q https://github.com/codingcog23/Segment-anything-using-Segment-Anything-Model/blob/main/data/dog-4.jpeg

"""## Load Model"""

import torch

DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
MODEL_TYPE = "vit_h"

from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor

sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)

"""## Automated Mask Generation

To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended.
"""

mask_generator = SamAutomaticMaskGenerator(sam)

import os

IMAGE_NAME = "humans.jpg"
IMAGE_PATH = os.path.join(HOME, "data", IMAGE_NAME)

"""### Generate masks with SAM for Images"""

import cv2
import supervision as sv

image_bgr = cv2.imread('/content/data/auto-wbg.jpg')
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

sam_result = mask_generator.generate(image_bgr)

"""### Output format

`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:

* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type
* `area` - `[int]` - the area of the mask in pixels
* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format
* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask
* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask
* `stability_score` - `[float]` - an additional measure of mask quality
* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format
"""

print(sam_result[0].keys())

"""### Results visualisation with Supervision

As of version `0.5.0` Supervision has native support for SAM.
"""

mask_annotator = sv.MaskAnnotator()

detections = sv.Detections.from_sam(sam_result=sam_result)

annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)

sv.plot_images_grid(
    images=[image_bgr, annotated_image],
    grid_size=(1, 2),
    titles=['source image', 'segmented image']
)

cv2.imwrite('/content/drive/MyDrive/Youtube Videos/Result/' + 'output2.png', annotated_image)

"""### Interaction with segmentation results"""

masks = [
    mask['segmentation']
    for mask
    in sorted(sam_result, key=lambda x: x['area'], reverse=True)
]

sv.plot_images_grid(
    images=masks,
    grid_size=(16, int(len(masks) / 8)),
    size=(16, 16)
)

"""## Generate Segmentation with Bounding Box

The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction.

Generate Generate masks with SAM for Videos
"""

def segment_object_video(image_bgr, output_path):
  # image_bgr = cv2.imread(IMAGE_PATH)
  image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

  sam_result = mask_generator.generate(image_rgb)
  mask_annotator = sv.MaskAnnotator()

  detections = sv.Detections.from_sam(sam_result=sam_result)

  annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)
  cv2.imwrite(output_path, annotated_image)
  sv.plot_images_grid(
    images=[image_bgr, annotated_image],
    grid_size=(1, 2),
    titles=['source image', 'segmented image'])

import cv2
def make_prediction_video(video_path):

  video = cv2.VideoCapture(video_path)
  count=0
  while True:
      res, frame = video.read()
      if not res:
          break  # Exit the loop when there are no more frames

      count += 1
      if count % 1 == 0:
          output_path = '/content/drive/MyDrive/Youtube_Videos/SAM/' + str(count).zfill(4) + ".png"
          segment_object_video(output_path=output_path, image_bgr=frame)

make_prediction_video(video_path='/content/data/cars.mp4')

import cv2
import glob

import cv2
import glob

def write_images_to_video(folder_path, output_video_path, fps=30.0):
    # Get the list of image paths inside the folder
    image_paths = glob.glob(folder_path + "/*.png")

    if len(image_paths) == 0:
        raise ValueError("The folder does not contain any image files.")

    # Read the first image to get its size
    first_image = cv2.imread(image_paths[0])
    height, width, _ = first_image.shape

    # Create a VideoWriter object
    fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')  # Try 'XVID' as the codec
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    # Write each image to the video
    for image_path in image_paths:
        image = cv2.imread(image_path)
        # image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR
        out.write(image)

    # Release the VideoWriter and close the video file
    out.release()


# Example usage
write_images_to_video(folder_path='/content/drive/MyDrive/Youtube_Videos/SAM',
                      output_video_path='/content/drive/MyDrive/Youtube_Videos/SAM/prediction2.mp4',
                      fps=30.0)